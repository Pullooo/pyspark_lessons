{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0caacdc-ab0b-4804-aab3-59e899421216",
   "metadata": {},
   "source": [
    "# Lesson 5 - Performance and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45917252-cd4e-49d9-95b5-fd120498a4d5",
   "metadata": {},
   "source": [
    "### 1. What is a Partition?\n",
    "- Huge datasets cannot fit into a single node. Hence have to be partitioned across different nodes\n",
    "- A partition in spark is basically and atomic chunk of data stored on a node in a cluster. They are the basic unit of parallelism\n",
    "- One partition cannot span over multiple machines\n",
    "- Spark automatically partitions RDDs/DataFrames and distributes the partitions across different nodes\n",
    "- We can configure the optimal number of partitions. Having too few/many partitions is not good\n",
    "\n",
    "\n",
    "`How Spark does the dafault partitioning of data`\n",
    "\n",
    "--> Spark checks HDFS block size for Hadoop(128MB for Hadoop 2.0/YARN) --> It creates one partition per block size (e.g. file of 500MB will have 4 partitions)\n",
    "\n",
    "\n",
    "` Why is it necessary to change partitions then?`\n",
    "- Spark partitions data (e.g. a file of size 2.6GB = 2.6 * 1024MB = 2662.4MB; 2662.4/128 = 20.8) into 21 partitions. \n",
    "- If you apply a filter on that dataframe which reduces it to a dataframe of size 1MB, spark will still consider it to have 21 partitions.\n",
    "- In this case, it'll make sense to reduce the partitions to reallocate resources and speed up data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c157446-3879-4b21-9266-4cfcf07721f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4ededb-3791-482b-b368-84ac00c90df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Training - DF APIs\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65aaed35-a26a-4ad0-b8f5-54766e8621cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rank: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Genre: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Director: string (nullable = true)\n",
      " |-- Actors: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Runtime (Minutes): string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- Votes: string (nullable = true)\n",
      " |-- Revenue (Millions): string (nullable = true)\n",
      " |-- Metascore: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('PracticeFiles/IMDB-Movie-Data.csv', header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411df84b-33a0-4d61-8915-633b7c963319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+------+------+------------------+---------+\n",
      "|Rank|               Title|               Genre|         Description|            Director|              Actors|Year|Runtime (Minutes)|Rating| Votes|Revenue (Millions)|Metascore|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+------+------+------------------+---------+\n",
      "|   1|Guardians of the ...|Action,Adventure,...|A group of interg...|          James Gunn|Chris Pratt, Vin ...|2014|              121|   8.1|757074|            333.13|       76|\n",
      "|   2|          Prometheus|Adventure,Mystery...|Following clues t...|        Ridley Scott|Noomi Rapace, Log...|2012|              124|     7|485820|            126.46|       65|\n",
      "|   3|               Split|     Horror,Thriller|Three girls are k...|  M. Night Shyamalan|James McAvoy, Any...|2016|              117|   7.3|157606|            138.12|       62|\n",
      "|   4|                Sing|Animation,Comedy,...|In a city of huma...|Christophe Lourdelet|Matthew McConaugh...|2016|              108|   7.2| 60545|            270.32|       59|\n",
      "|   5|       Suicide Squad|Action,Adventure,...|A secret governme...|          David Ayer|Will Smith, Jared...|2016|              123|   6.2|393727|            325.02|       40|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+----+-----------------+------+------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb04e8e-0b1e-4308-a52b-96874933ee9c",
   "metadata": {},
   "source": [
    "### 1.2 Repartition(numPartitions, *cols):\n",
    "- *cols: optional\n",
    "- dataframe is hash partitioned\n",
    "- creates almost equal sized partitions\n",
    "- can increase or decrease the level of parallelism\n",
    "- Internally, this redistributes data from all partitions leading to a very expensive operation. So avoid if not required\n",
    "- `Note:` Spark performs better with equal sized partitions. If you need further processing of huge data, it is preferred to have equal sized partitions, so worth considering\n",
    "- If you are decreasing teh number of partitions, consider using `coalese`, since this minimises movement od data across partitions and doesn't try creating equal size partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937af9f-9894-40cb-bfef-23b9bd8fc1ca",
   "metadata": {},
   "source": [
    "<img src=\"repartition.png\" style=\"width:400px; height:400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d429fa20-8682-420d-adbb-a68e77d0190b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d65c3b46-d7ae-4b83-aef6-e8790c83a8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c950f3dc-ddca-40ba-a5ad-504c4c54ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: create new dataframe with more partitions\n",
    "df_new = df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e172dea-6525-48bc-8b5a-0e0a968074c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8483a09b-9b6b-4784-965a-fd96866bea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200, 200, 200, 200, 200]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many rows per partition\n",
    "df_new.rdd.glom().map(len).collect()\n",
    "# Notice it creates equal size partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0693c7c2-5025-40f1-aed4-fb40e1ce17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Create a new dataframe partitioned on Year\n",
    "df_partitioned_on_yrs = df.repartition('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe958735-1d70-41f4-a9f5-ddec632d4d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice it will give it a default of 200 partitions\n",
    "df_partitioned_on_yrs.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "487ce9bc-c28b-4da3-8489-cac8c60f75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets partitions by Year and give it 4 partitions\n",
    "df_partitioned_on_yrs = df.repartition(4, 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0b7f26e-33b8-4cc6-bca4-5314bfc94cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_partitioned_on_yrs.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b90f3c-f8b4-4e39-94e3-a854f8273884",
   "metadata": {},
   "source": [
    "### 1.3 coalesce(numPartitions)\n",
    "- Returns a new dataframe that is reduced into numPartitions partitions\n",
    "- optimised version of repartition()\n",
    "- no shuffling\n",
    "- Results in a narrow dependency e.g. if you go from 1000 partitions to 100 partitions, because there's no shuffle, you'll get 100 new partitions that will claim 10 of the current partitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae05122-501e-4a88-9e94-b76a3e8422d0",
   "metadata": {},
   "source": [
    "<img src=\"coalesce.png\" style=\"width:400px; height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324a19f-32fd-468d-b032-a33d5527a0b6",
   "metadata": {},
   "source": [
    "From above diagram, we see that coalese when used to reduce partitions e.g. from 4 to 2:\n",
    "- does not create a new partition\n",
    "- It uses existing partitions p1 and p3, and tries to move data from partitions p2 into p1, and from partitions p4 into p3\n",
    "- p1 now contains data for p1 and p2\n",
    "- p3 now contains data for p3 and p4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6738b100-1cbc-48af-b4df-89ea1e56eb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200, 200, 200, 200, 200]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check numb of partitions in df_new\n",
    "df_new.rdd.glom().map(len).collect()\n",
    "# Notice it creates equal size partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd6be9-fa84-4073-a894-272bc7cc2172",
   "metadata": {},
   "source": [
    "##### repartiton vs coalesce to reduce df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d579ff2d-2a05-4757-a048-b94b1670a917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[333, 334, 333]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using repartition\n",
    "df_new_repart = df_new.repartition(3)\n",
    "\n",
    "df_new_repart.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04844899-d456-4886-a564-e3e0c5dea096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200, 400, 400]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using coalesce\n",
    "df_new_coalesce = df_new.coalesce(3)\n",
    "\n",
    "df_new_coalesce.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef50532-5463-450d-b330-92d0eab75f56",
   "metadata": {},
   "source": [
    "`Notice:`\n",
    "- repartition creates equal sized partitions\n",
    "- Coalese creates unequal sized partitions\n",
    "\n",
    "`also notice: next`:\n",
    "- with coalese you can't increase the number of partitions. It'll just stay unchanged\n",
    "- you can only decrease number of partitions with coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7b82275-64f2-4c35-8f06-375cd47de2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[200, 200, 200, 200, 200]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's increase number of partitions from 5 to 6\n",
    "df_new3 = df_new.coalesce(6)\n",
    "\n",
    "df_new3.rdd.glom().map(len).collect()\n",
    "# There'll be no change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0c27d-29fe-4d22-8fa7-bbf984d8fe6d",
   "metadata": {},
   "source": [
    "### repartition vs coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1d976-e744-4cb1-ae06-7272b8ca1891",
   "metadata": {},
   "source": [
    "<img src=\"comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801add3-6972-4834-bcdd-21b64f2e521a",
   "metadata": {},
   "source": [
    "<img src=\"comparison.png\" style=\"width:500px; height:00px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c703531e-7815-4b19-adb3-800e81b49ccb",
   "metadata": {},
   "source": [
    "## 2. Performance Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d60e65-e6c2-4578-afc6-eb316c5b893d",
   "metadata": {},
   "source": [
    "### 2.1 Join Strategies\n",
    "Spark has the below important join strategies\n",
    "- Broadcast join (hint - Broadcast)\n",
    "- Shuffle Hash join (hint - shuffle_hash)\n",
    "- Sort Merge join (hint - sort merge)\n",
    "- Cartesian Product Join (Hint - Broadcast)\n",
    "- Broadcast Nested Loop Join (Hint - Shuffle_replicate_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c795d-67cd-4009-9f16-f647bb7cf4e3",
   "metadata": {},
   "source": [
    "#### i. Broadcast join\n",
    "- This is one of the most powerful performance optimization technique we can use\n",
    "- Performs a join in 2 steps:\n",
    "\n",
    "<img src='broadcast.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6e94be-fbba-4c06-9e33-53ebd8854545",
   "metadata": {},
   "source": [
    "Let's say we are trying to join a massive table (table A) to a small table - table B.\n",
    "- `Step 1:` The driver will send table B data to all executors where Table A's partitions are present using a BitTorrent Protocol (basically driver sends it to a worker node, which sends a copy to each worker node\n",
    "- `Step 2:` It then performs a hash join between the partitions and table B\n",
    "\n",
    "`Notes`\n",
    "- In this way, all executors have all information required to perform the join at it's location, without needing to redistribute data and shuffle\n",
    "- Broadcast join can be very efficient join between a large table (fact table) and a relatively small table (dimensions table) that could then be used to perform a star-schema join\n",
    "- Size of the smaller table should be less than: `spark.sql.autoBroadcastJoinThreshold.` Configurable Default size: 10MB - `int(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))/1024/1024`\n",
    "- Recently Spark increased the max size for the broadcast table from 2GB to 8GB. Thus, it's not possible to broadcast tables greater than 8GB in size. (so we can increase default size to 8GB)\n",
    "- aka replicated join since smaller df is replicated to all the executors\n",
    "- use Hint `BROADCAST` to force Spark Optimizer to perform broadcast join\n",
    "\n",
    "#### 2.11 Auto Detection\n",
    "- In many cases, Spark can automatically detect whether to use a broadcast join or not, depending on the size of the data.\n",
    "- If spark detects that one of the joined dataframes is amall (10 MB by default), it will automatically broadcast it for us\n",
    "\n",
    "`Note`: Spark will only perform autodetection in the following instances:\n",
    "\n",
    "1. Spark constructs the dataframe from scratch e.g. using spark.range\n",
    "2. It reads from files with schema and/or size info e.g. Parquet, Avro\n",
    "\n",
    "The reason is because spark can detect the sizes of these files easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da70524-f861-418a-bd91-9b2c2f00a871",
   "metadata": {},
   "source": [
    "### 2.12 Testing\n",
    "Follow the following steps to test your joins:\n",
    "1. Test the joins with and without the presence of auto optimisation by turning it off i.e. set `spark.sql.autoBroadcastJoinThreshold` to -1\n",
    "2. Compare run times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06665de6-156e-40d2-be78-eb5d4a05c4d7",
   "metadata": {},
   "source": [
    "#### 2.13 Further Notes:\n",
    "- broadcast joins support all join types (inner, left, right) except full outer join\n",
    "- Faster than any other join strategies\n",
    "- Only supported for '=' join\n",
    "- broadcast table needs to be < 10MB in size (default). Can be increased to 8GB\n",
    "\n",
    "#### 2.13 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83596c26-6ec7-4562-841f-1128387a1d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check default value (in bytes) of this broadcast join threshold\n",
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6bf0c3-f669-4208-8d74-aa074aa59480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets convert that to MB\n",
    "int(spark.conf.get('spark.sql.autoBroadcastJoinThreshold')[:8])/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24b57cc8-c774-41ab-89b7-052ceac2bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ord = spark.read.load('PracticeFiles/Orders', sep=',', format='csv', schema=('order_id int,order_date timestamp, order_customer_id int, order_status string'))\n",
    "ord.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88660b7e-4d48-4f0f-81b1-12aa913b230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+--------+--------+------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|quantity|subtotal| price|\n",
      "+-------------+-------------------+---------------------+--------+--------+------+\n",
      "|            1|                  1|                  957|       1|  299.98|299.98|\n",
      "|            2|                  2|                 1073|       1|  199.99|199.99|\n",
      "|            3|                  2|                  502|       5|   250.0|  50.0|\n",
      "+-------------+-------------------+---------------------+--------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_item = spark.read.load('PracticeFiles/Order_items', sep=',', format='csv', schema=('order_item_id int, order_item_order_id int, order_item_product_id int, quantity tinyint, subtotal float, price float'))\n",
    "order_item.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91a6420b-771c-473f-8220-be9ad265bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = ord.join(order_item, ord.order_id == order_item.order_item_order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23b72642-5ed2-4b40-b0e7-8f6618529cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+-------------+-------------------+---------------------+--------+--------+------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|order_item_id|order_item_order_id|order_item_product_id|quantity|subtotal| price|\n",
      "+--------+-------------------+-----------------+---------------+-------------+-------------------+---------------------+--------+--------+------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|            1|                  1|                  957|       1|  299.98|299.98|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|            2|                  2|                 1073|       1|  199.99|199.99|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|            3|                  2|                  502|       5|   250.0|  50.0|\n",
      "+--------+-------------------+-----------------+---------------+-------------+-------------------+---------------------+--------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e7673c6-ac34-44b4-82d1-682346ccf8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [order_id#190], [order_item_order_id#448], Inner, BuildLeft, false\n",
      ":- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#265]\n",
      ":  +- *(1) Filter isnotnull(order_id#190)\n",
      ":     +- FileScan csv [order_id#190,order_date#191,order_customer_id#192,order_status#193] Batched: false, DataFilters: [isnotnull(order_id#190)], Format: CSV, Location: InMemoryFileIndex[file:/Users/paulfru/Desktop/projects/pyspark/PracticeFiles/Orders], PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,order_date:timestamp,order_customer_id:int,order_status:string>\n",
      "+- *(2) Filter isnotnull(order_item_order_id#448)\n",
      "   +- FileScan csv [order_item_id#447,order_item_order_id#448,order_item_product_id#449,quantity#450,subtotal#451,price#452] Batched: false, DataFilters: [isnotnull(order_item_order_id#448)], Format: CSV, Location: InMemoryFileIndex[file:/Users/paulfru/Desktop/projects/pyspark/PracticeFiles/Order_items], PartitionFilters: [], PushedFilters: [IsNotNull(order_item_order_id)], ReadSchema: struct<order_item_id:int,order_item_order_id:int,order_item_product_id:int,quantity:tinyint,subto...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's explain the join plan\n",
    "joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f034e12-4b24-446a-a2ed-818fc3440fe2",
   "metadata": {},
   "source": [
    "- On the second line, we see it gets both files and applies broadcastHashjoin. So looks fine\n",
    "\n",
    "`Let's turn auto optimisation off`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61712d79-af72-4f9d-8485-14322e78e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn optimisation off by setting threshold to -1\n",
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "467b3dce-2ce3-46bc-bbf8-02d6f726c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [order_id#190], [order_item_order_id#448], Inner\n",
      ":- *(2) Sort [order_id#190 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(order_id#190, 200), ENSURE_REQUIREMENTS, [id=#300]\n",
      ":     +- *(1) Filter isnotnull(order_id#190)\n",
      ":        +- FileScan csv [order_id#190,order_date#191,order_customer_id#192,order_status#193] Batched: false, DataFilters: [isnotnull(order_id#190)], Format: CSV, Location: InMemoryFileIndex[file:/Users/paulfru/Desktop/projects/pyspark/PracticeFiles/Orders], PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,order_date:timestamp,order_customer_id:int,order_status:string>\n",
      "+- *(4) Sort [order_item_order_id#448 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(order_item_order_id#448, 200), ENSURE_REQUIREMENTS, [id=#308]\n",
      "      +- *(3) Filter isnotnull(order_item_order_id#448)\n",
      "         +- FileScan csv [order_item_id#447,order_item_order_id#448,order_item_product_id#449,quantity#450,subtotal#451,price#452] Batched: false, DataFilters: [isnotnull(order_item_order_id#448)], Format: CSV, Location: InMemoryFileIndex[file:/Users/paulfru/Desktop/projects/pyspark/PracticeFiles/Order_items], PartitionFilters: [], PushedFilters: [IsNotNull(order_item_order_id)], ReadSchema: struct<order_item_id:int,order_item_order_id:int,order_item_product_id:int,quantity:tinyint,subto...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets re-perform join and check join plan\n",
    "joined = ord.join(order_item, ord.order_id == order_item.order_item_order_id)\n",
    "\n",
    "joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da9150-26f5-4084-848f-94398be04d4e",
   "metadata": {},
   "source": [
    "#### Example 2: Lets create two dataframes and see if spark can figure out whether to broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85512cd6-068c-4fbb-b3f5-e37b6ce14477",
   "metadata": {},
   "outputs": [],
   "source": [
    "largDF = spark.range(1, 1_000_000_000)\n",
    "data = [(1, 'a'),(2, 'b'), (3, 'c')]\n",
    "schema = ['id', 'col2']\n",
    "smallDF = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2813321-2265-4d86-8f5f-6fa32873aca9",
   "metadata": {},
   "source": [
    "- In theory if optimizer can figure out the sizes of both the large and small df, it will apply broadcast join\n",
    "- But if small dataframe is created on top of local collection, spark wont know whether to apply broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebdd3710-675d-46d4-bafd-7ac90da44b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold',10485760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "439bf3af-9e02-48dc-bacc-30ca3702508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [id#601L, col2#604]\n",
      "+- *(5) SortMergeJoin [id#601L], [id#603L], Inner\n",
      "   :- *(2) Sort [id#601L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#601L, 200), ENSURE_REQUIREMENTS, [id=#342]\n",
      "   :     +- *(1) Range (1, 1000000000, step=1, splits=8)\n",
      "   +- *(4) Sort [id#603L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#603L, 200), ENSURE_REQUIREMENTS, [id=#348]\n",
      "         +- *(3) Filter isnotnull(id#603L)\n",
      "            +- *(3) Scan ExistingRDD[id#603L,col2#604]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joindf = largDF.join(smallDF, 'id')\n",
    "# Notice broadcast join isnt used\n",
    "joindf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f4ef097-a1ea-495e-92ca-dad4797befa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [id#603L, col2#604]\n",
      "+- *(2) BroadcastHashJoin [id#603L], [id#601L], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#379]\n",
      "   :  +- *(1) Filter isnotnull(id#603L)\n",
      "   :     +- *(1) Scan ExistingRDD[id#603L,col2#604]\n",
      "   +- *(2) Range (1, 1000000000, step=1, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can instruct optimizer to use broadcast join here\n",
    "joindf = smallDF.hint(\"BROADCAST\").join(largDF, 'id')\n",
    "# take note of the ordering of the tables\n",
    "joindf.explain()\n",
    "# now it uses broadcast join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc1720-9244-4d95-885f-e2867091b49a",
   "metadata": {},
   "source": [
    "## 3. Driver Configurations\n",
    "\n",
    "### 3.1 Driver Options\n",
    "- When we apply collect(), take() operations on datasets, it requires the data to be moved to Driver. If we do so on huge datasets, it can crash the driver process with Out of Memory erros (OOM).\n",
    "- If you oberve we perform most of the computational work of a Spark Job in the executors and so rarely require to do any performance tuning for the driver\n",
    "- However, sometimes the job might fail if we collect too much data to the driver.\n",
    "- Setting a proper limit can protect the driver from out of memory errors.\n",
    "\n",
    "### 3.2 Spark-submit Options\n",
    "1. `driver-memory:` Momory for the driver (e.g. 1000M, 2G). Default = 1024\n",
    "- Driver memory is the amount of memory to use for driver process i.e. the process running the main() function of the application and where SparkContext is instantiated\n",
    "2. `Driver cores:`\n",
    "- Number of cores used by the driver, only in cluster mode (default: 1)\n",
    "- Generally not required unless you want to perform some local computations in parallel\n",
    "\n",
    "### 3.3 spark.driver.maxResultSize\n",
    "- Limit of each Spark action (e.g. collect) in bytes\n",
    "- Should be atleast 1M, or 0 for unlimited. Jobs will be aborted if the total size is above this limit\n",
    "- Having a high limit may cause Out-of-memory erros in driver (depends on spark.driver.memory and memory overhead of objects in JVM)\n",
    "\n",
    "### 3.4 spark.driver.memoryOverhead\n",
    "- Amount of overhead (non-heap memory to be allocated per driver process in cluster mode, in MiB unless otherwise specified\n",
    "- This is memory that accounts for things like VM overheads, interned strings, other native overheads etc\n",
    "- This tends to grow tith the container size (typically 6-10%)\n",
    "\n",
    "### 3.4 Summary plus other properties\n",
    "- spark.driver.memory: Default 1024\n",
    "- spark.driver.cores: Default 1\n",
    "- spark.driver.maxResultSize\n",
    "- spark.driver.memoryOverhead: Default driver Momory * 0.10 with minimum of 384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6014d4-3408-4bd3-acb6-e58cb5f4712b",
   "metadata": {},
   "source": [
    "## 4. Excutor Configurations\n",
    "\n",
    "<img src='driverprog.png'>\n",
    "\n",
    "<img src='executors.png'>\n",
    "\n",
    "### Executors and Cores:\n",
    "- Executors are created in worker/data nodes and thay are in charge of running tasks in a given job\n",
    "- Each executor comprises a JVM (for each executor, one JVM process is created). They are launched at the beginning of a spark application and run the entire lifetime of the spark job\n",
    "- After they run the assigned task, they send the results to the driver\n",
    "- They also provide in-memory storage for RDDs that are cached by user programs\n",
    "- Each worker node can have multiple cores\n",
    "- To run the tasks in parallel, we can launch executors with multiple cores\n",
    "\n",
    "\n",
    "### How do we configure them\n",
    "Below are some helpful configs for executors\n",
    "- `spark.executor.memory:` Default = 1G\n",
    "- `spark.executor.cores:` Default = 2\n",
    "- `spark.executor.memoryOverhead:` 10% of 384MB (whichever is higher)\n",
    "--> The amount of off-heap memory to be allocated per executor, in MiB unless otherwise specified.\n",
    "--> This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc.\n",
    "--> This tends to grow with the executor size (typically 6-10%).\n",
    "-->When we plan the performance tuning we need to consider this as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a905c65-3bfd-42af-8659-d951a7afb742",
   "metadata": {},
   "source": [
    "#### Spark runtime components in cluster deploy mode\n",
    "<img src='clusterdeploymode.png'>\n",
    "\n",
    "\n",
    "#### Spark runtime components in client deploy mode\n",
    "<img src='clientdeploymode.png'>\n",
    "\n",
    "#### node manager\n",
    "<img src='nodemanager.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdbf791-6645-4b59-a546-1acf10621472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
